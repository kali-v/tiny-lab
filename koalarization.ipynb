{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73a6f65",
   "metadata": {},
   "source": [
    "**Deep Koalarization (https://arxiv.org/pdf/1712.03400.pdf)** \\\n",
    "Deep Koalarization proposes a convolutional network that uses Inception-Resnet-v2 trained on ImageNet as a feature extractor. The network itself consists of three main parts, which are an encoder, fusion (with the result of Inception-Resnet-v2), and decoder. The output of the network are also a* and b* layers in CIE Lab color space. Deep Koalarization paper uses MSE Loss, Adam optimizer and the training set consists of ~54000 images from ImageNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3021fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html --no-cache-dir\n",
    "! pip install scikit-image matplotlib validators tensorboard setuptools==59.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa6a0e",
   "metadata": {},
   "source": [
    "# Data download\n",
    "\n",
    "I used COCO because it's more accessible than ImageNet in og paper. I'm using test2017 + test2014 as the training set(80k images) and val2017 as the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/zips/test2017.zip -P persistent-storage\n",
    "!wget http://images.cocodataset.org/zips/test2014.zip -P persistent-storage\n",
    "!wget http://images.cocodataset.org/zips/val2017.zip -P persistent-storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd39f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('data/test2017.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/coco')\n",
    "\n",
    "!mv data/coco/test2017/* data/coco\n",
    "!rmdir data/coco/test2017\n",
    "\n",
    "with zipfile.ZipFile('data/test2014.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/coco-tmp')\n",
    "\n",
    "!mv data/coco-tmp/test2014/* data/coco\n",
    "!rmdir data/coco-tmp\n",
    "\n",
    "with zipfile.ZipFile('data/val2017.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/coco-val')\n",
    "\n",
    "!mv data/coco-val/val2017/* data/coco-val\n",
    "!rmdir data/coco-val/val2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15708794",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import io\n",
    "\n",
    "# remove grayscale images\n",
    "for i, image_path in enumerate(os.listdir('persistent-storage/coco')):\n",
    "    im_path = f'persistent-storage/coco/{image_path}'\n",
    "    image = io.imread(im_path)\n",
    "    if image.shape[-1] != 3:\n",
    "        os.remove(im_path)\n",
    "\n",
    "for i, image_path in enumerate(os.listdir('persistent-storage/coco-val')):\n",
    "    im_path = f'persistent-storage/coco-val/{image_path}'\n",
    "    image = io.imread(im_path)\n",
    "    if image.shape[-1] != 3:\n",
    "        os.remove(im_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78414645",
   "metadata": {},
   "source": [
    "# Conversion to LAB color model\n",
    "\n",
    "The paper uses LAB color model. The input is luminescense component and the network is trying to predict a* and b* components. All components are further normalized to values between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageCms\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "im = Image.open('/persistent-storage/coco/000000096944.jpg').convert('RGB')\n",
    "\n",
    "a = np.array(im) / 255\n",
    "\n",
    "lab = rgb2lab(a)\n",
    "l, a, b = np.moveaxis(lab, -1, 0)\n",
    "\n",
    "f, ax = plt.subplots(2, 2)\n",
    "\n",
    "ax[0][0].imshow(im)\n",
    "ax[0][1].imshow(l, cmap='gray')\n",
    "ax[1][0].imshow(a, cmap='gray')\n",
    "\n",
    "ax[1][1].set_title('blueyellow')\n",
    "ax[1][1].imshow(b, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2cbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_comp_stats(l, a, b):\n",
    "    print(f\"L: min: {np.min(l)} max: {np.max(l)}\")\n",
    "    print(f\"A: min: {np.min(a)} max: {np.max(a)}\")\n",
    "    print(f\"B: min: {np.min(b)} max: {np.max(b)}\")\n",
    "\n",
    "lab_comp_stats(l,a,b)\n",
    "\n",
    "nl = np.interp(l, (0, 100), (-1, +1))\n",
    "na = np.interp(a, (-128, 127), (-1, +1))\n",
    "nb = np.interp(b, (-128, 127), (-1, +1))\n",
    "\n",
    "lab_comp_stats(nl,na,nb)\n",
    "\n",
    "nnl = np.interp(nl, (-1, +1), (0, 100))\n",
    "nna = np.interp(na, (-1, +1), (-128, 127))\n",
    "nnb = np.interp(nb, (-1, +1), (-128, 127))\n",
    "\n",
    "lab_comp_stats(nnl,nna,nnb)\n",
    "\n",
    "rgb = lab2rgb(np.moveaxis(np.array([nnl, nna, nnb]), 0, -1))\n",
    "plt.imshow(rgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57624e35",
   "metadata": {},
   "source": [
    "Dataset is loading images from directory and preprocessing them so they can be sent directly to model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageCms\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac45dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from skimage import io, transform\n",
    "from skimage.color import rgb2lab\n",
    "import glob\n",
    "\n",
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, root_dir, device, transforms):\n",
    "        self.root_dir = root_dir\n",
    "        self.device = device\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.images = glob.glob(f'{self.root_dir}/*.jpg')\n",
    "        self.len = len(self.images)\n",
    "\n",
    "    def get_image(self, idx):\n",
    "        image = io.imread(f'{self.images[idx]}')\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def preprocess_image(self, image):\n",
    "        l, a, b = np.moveaxis(rgb2lab(image), -1, 0)\n",
    "\n",
    "        # normalize image\n",
    "        l = np.interp(l, (0, 100), (-1, +1))\n",
    "        a = np.interp(a, (-128, 127), (-1, +1))\n",
    "        b = np.interp(b, (-128, 127), (-1, +1))\n",
    "\n",
    "        x = torch.Tensor(l).unsqueeze(0)\n",
    "        y = torch.Tensor([a,b])\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.get_image(idx)\n",
    "        return self.preprocess_image(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825d762",
   "metadata": {},
   "source": [
    "# Unet baseline\n",
    "\n",
    "I used Unet networks as baseline, I tried to use both smaller version (UNetMini) and also the full version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad921383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class UNetMini(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(UNetMini, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(192, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(96, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.out_conv = nn.Conv2d(32, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.block1(x)\n",
    "        out_pool1 = nn.MaxPool2d((2, 2))(out1)\n",
    "\n",
    "        out2 = self.block2(out_pool1)\n",
    "        out_pool2 = nn.MaxPool2d((2, 2))(out2)\n",
    "\n",
    "        out3 = self.block3(out_pool2)\n",
    "        out_up1 = nn.Upsample(scale_factor=2)(out3)\n",
    "\n",
    "        out4 = torch.cat((out_up1, out2), dim=1)\n",
    "        out4 = self.block4(out4)\n",
    "\n",
    "        out_up2 = nn.Upsample(scale_factor=2)(out4)\n",
    "        out5 = self.block5(torch.cat((out_up2, out1), dim=1))\n",
    "\n",
    "        return self.out_conv(out5)\n",
    "\n",
    "# Full unet\n",
    "unet_model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=1, out_channels=2, init_features=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1e004",
   "metadata": {},
   "source": [
    "# Deep Koalarization\n",
    "\n",
    "Deep koalarization uses pre-trained Inception-ResNet-v2 as a feature extractor. Because Efficient-Net B4 is smaller, quicker, and better performing on ImageNet a decided to use this network instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8917cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class KoalaNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 256, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.efficient_net = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b4', pretrained=True).to(\"cuda\")\n",
    "        for param in self.efficient_net.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(1256, 256, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 2, 3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = len(x)\n",
    "\n",
    "        enc_ten = self.encoder(x)\n",
    "\n",
    "        # fusion\n",
    "        feat_ten = self.efficient_net(x.repeat(1, 3, 1, 1))\n",
    "        feat_ten = torch.permute(feat_ten.repeat(28, 28, 1, 1), (2, 3, 1, 0))\n",
    "        fused_ten = torch.cat((enc_ten, feat_ten), dim=1)\n",
    "\n",
    "        out_ten = self.decoder(fused_ten)\n",
    "\n",
    "        return out_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a8829",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Even though the original DeepKoalarization paper doesn't mention any data augmentation I decided to use RandomHorizontalFlip and random cropping which seemed to help generalize over the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((268, 268)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc781b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataset = ColorizationDataset('persistent-storage/coco', \"cuda\", train_transforms)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 1000, 1000])\n",
    "train_dataloader = DataLoader(train_dataset, num_workers=2, batch_size=32, shuffle=True, prefetch_factor=2)\n",
    "val_dataloader = DataLoader(val_dataset, num_workers=1, batch_size=32) # 1000 images as validation set\n",
    "\n",
    "test_dataset = ColorizationDataset('persistent-storage/coco-val', \"cuda\", test_transforms)\n",
    "test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc23b40",
   "metadata": {},
   "source": [
    "Main training cycle. The training ran for about 10 hours (70 epochs) on 80k training images (on A10/A40? - can't recall). Even though the loss at the end was still slightly dropping, the visual difference between the last 5 epochs was minimal. All trained models can be found at: https://drive.google.com/drive/folders/1Ey-ZRnMkdMVf5soanxvMEVzZcXTqI5Kz?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KoalaNet()\n",
    "model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f38b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gc\n",
    "\n",
    "epoch_size = len(train_dataset) / batch_size\n",
    "\n",
    "writer = SummaryWriter()\n",
    "total_losses = []\n",
    "total_ssim_list= []\n",
    "total_loss_list = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    i = 0\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in train_dataloader:\n",
    "        i += 1\n",
    "        x = x.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(x)\n",
    "\n",
    "        loss = loss_fn(out, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            rloss = total_loss / i\n",
    "            print(f\"running loss [{i}/{epoch_size}]: {rloss}\")\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "        optimizer.step()\n",
    "    \n",
    "    total_losses.append(total_loss)\n",
    "    torch.save(model.state_dict(), f'persistent-storage/unet.pth')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_ssim = 0\n",
    "        total_loss = 0\n",
    "        for x, y in val_dataloader:\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "            out = model(x).cpu()\n",
    "            \n",
    "            total_loss += loss_fn(out, y)\n",
    "\n",
    "            out = np.interp(out, (-1, +1), (0, 1))\n",
    "            y = np.interp(y, (-1, +1), (0, 1))\n",
    "            \n",
    "            for b in range(len(out)):\n",
    "                total_ssim += 2 - ssim(out[b][0], y[b][0]) - ssim(out[b][1], y[b][1])\n",
    "        \n",
    "        print(f'val inv ssim: {total_ssim} val loss: {total_loss}')\n",
    "        total_ssim_list.append(total_ssim)\n",
    "        total_loss_list.append(total_loss)\n",
    "        writer.add_scalar('ssim/val', total_ssim, epoch)\n",
    "        writer.add_scalar('loss/val', total_loss, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f61c8",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Most papers evaluate their models according to the success with which they can fool the person who is guessing which image is original to the grayscale image. If humans prefer colorized images over the ground truths then the model is considered better performing. Also, it must be taken into account that not all colorization methods are heading in the same direction. Some models try to solve the overly conservative guessing and try to guess colors \"more aggressively\", some just try to match reality as close as possible without any artistic effects, etc. Because of these reasons, it's hard to come up with a universal mathematical function to evaluate colorization methods.\n",
    "\n",
    "Some of the used methods are a Peak signal-to-noise ratio (PSNR) often used alongside compression and Structural similarity (SSIM) also used in compression, pattern recognition, and image restoration. In the end, I used SSIM to validate progress on the validation set at the end of every epoch.\n",
    "\n",
    "Even though the function isn't universal, this way we can at least ensure that the model is going the right way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f78618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "# experiments with ssim, we can see the degredation curve when applying noise\n",
    "\n",
    "x, y = val_dataset[0]\n",
    "\n",
    "y = y.detach().cpu().numpy()\n",
    "x = x.detach().cpu()[0].numpy()\n",
    "\n",
    "l = np.interp(x, (-1, +1), (0, 100))\n",
    "y0 = np.interp(y[0], (-1, +1), (0, 1))\n",
    "yy0 = y0 * 0.5\n",
    "y1 = np.interp(y[1], (-1, +1), (-128, 127))\n",
    "\n",
    "print(ssim(y0, y0 * 0.1))\n",
    "print(ssim(y0, y0 * 0.4))\n",
    "print(ssim(y0, y0 * 0.8))\n",
    "print(ssim(y0, y0 * 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72350b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_ssim = 0\n",
    "        total_loss = 0\n",
    "        for x, y in val_dataloader:\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "            out = model(x).cpu()\n",
    "            \n",
    "            total_loss += loss_fn(out, y)\n",
    "\n",
    "            out = np.interp(out, (-1, +1), (0, 1))\n",
    "            y = np.interp(y, (-1, +1), (0, 1))\n",
    "            \n",
    "            for b in range(len(out)):\n",
    "                total_ssim += 2 - ssim(out[b][0], y[b][0]) - ssim(out[b][1], y[b][1])\n",
    "        \n",
    "        print(f'val inv ssim: {total_ssim} val loss: {total_loss}')\n",
    "        total_ssim_list.append(total_ssim)\n",
    "        total_loss_list.append(total_loss)\n",
    "\n",
    "\n",
    "koalamodel = KoalaNet().cuda()\n",
    "koalamodel.load_state_dict(torch.load('./persistent-storage/final-koala.pth'))\n",
    "\n",
    "unetminimodel = UNetMini().cuda()\n",
    "unetminimodel.load_state_dict(torch.load('./persistent-storage/unet-mini.pth'))\n",
    "\n",
    "validate_model(koalamodel)\n",
    "validate_model(unetminimodel)\n",
    "validate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_unseen_image(index, model=koalamodel):\n",
    "    x, y = test_dataset[index]\n",
    "    x = x.to(\"cuda\")\n",
    "    \n",
    "    pa, pb = model(x.resize(1, 1, 224, 224))[0]\n",
    "\n",
    "    y = y.detach().cpu().numpy()\n",
    "    x = x.detach().cpu().numpy()\n",
    "    pa = pa.detach().cpu().numpy()\n",
    "    pb = pb.detach().cpu().numpy()\n",
    "\n",
    "    # scale back from -1 to 1 to LAB values\n",
    "    l = np.interp(x, (-1, +1), (0, 100))\n",
    "    a = np.interp(pa, (-1, +1), (-128, 127))\n",
    "    b = np.interp(pb, (-1, +1), (-128, 127))\n",
    "\n",
    "    y0 = np.interp(y[0], (-1, +1), (-128, 127))\n",
    "    y1 = np.interp(y[1], (-1, +1), (-128, 127))\n",
    "\n",
    "    l = np.squeeze(l, 0)\n",
    "\n",
    "    orig = np.array([l, y0, y1])\n",
    "    origrgb = lab2rgb(np.moveaxis(orig, 0, -1))\n",
    "\n",
    "    lab = np.array([l, a, b])\n",
    "    rgb = lab2rgb(np.moveaxis(lab, 0, -1))\n",
    "\n",
    "    return l, origrgb, rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1709fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f, ax = plt.subplots(1, 5, figsize=(20,20))\n",
    "im1 = predict_unseen_image(78, unetminimodel)\n",
    "im2 = predict_unseen_image(78, unet_model)\n",
    "im3 = predict_unseen_image(78, koalamodel)\n",
    "ax[0].imshow(im1[0], cmap='gray')\n",
    "ax[1].imshow(im1[1])\n",
    "ax[2].imshow(im1[2])\n",
    "ax[3].imshow(im2[2])\n",
    "ax[4].imshow(im3[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eea902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2lab, lab2rgb\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"SHOWCASE\")\n",
    "print(\"each row contains three images: grayscale, original and predicted from grayscale respectively\")\n",
    "print(\"all presented images are taken from test set and thus was never seenby the model\")\n",
    "print(\"-----\")\n",
    "print(\"Common problem when colorizing images is deciding which color use when the color of the object may not be clear\")\n",
    "qq = [1152, 1159, 1123]\n",
    "f, ax = plt.subplots(len(qq), 3, figsize=(10,10))\n",
    "for i in range(len(qq)):\n",
    "    im1 = predict_unseen_image(qq[i])\n",
    "    ax[i][0].imshow(im1[0], cmap='gray')\n",
    "    ax[i][1].imshow(im1[1])\n",
    "    ax[i][2].imshow(im1[2])\n",
    "plt.show()\n",
    "\n",
    "print(\"Another common problem is colorizing very small objects or image with too many objects in the scene\")\n",
    "qq = [34, 78, 1098]\n",
    "f, ax = plt.subplots(len(qq), 3, figsize=(10,10))\n",
    "for i in range(len(qq)):\n",
    "    im1 = predict_unseen_image(qq[i])\n",
    "    ax[i][0].imshow(im1[0], cmap='gray')\n",
    "    ax[i][1].imshow(im1[1])\n",
    "    ax[i][2].imshow(im1[2])\n",
    "plt.show()\n",
    "\n",
    "print(\"On the other hand, common themes like nature are often the more easier ones.\")\n",
    "print(\"But, the same problems remains even there e.g. the small girrafes get little bit green by trees behind them, from this same problem suffers also the original DeepKoalarizationNet\")\n",
    "qq = [738, 32, 712, 1046, 186, 1136]\n",
    "f, ax = plt.subplots(len(qq), 3, figsize=(20,60))\n",
    "for i in range(len(qq)):\n",
    "    im1 = predict_unseen_image(qq[i])\n",
    "    ax[i][0].imshow(im1[0], cmap='gray')\n",
    "    ax[i][1].imshow(im1[1])\n",
    "    ax[i][2].imshow(im1[2])\n",
    "plt.savefig('third.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Also other examples show that network often gets the general idea correctly but still missing precission\")\n",
    "\n",
    "q = [213, 834, 513, 93, 425, 1026, 1037, 943, 350, 1039]\n",
    "# 934, 6\n",
    "# 943, 350\n",
    "f, ax = plt.subplots(len(q), 3, figsize=(20,50))\n",
    "for i in range(len(q)):\n",
    "    im1 = predict_unseen_image(q[i])\n",
    "    ax[i][0].imshow(im1[0], cmap='gray')\n",
    "    ax[i][1].imshow(im1[1])\n",
    "    ax[i][2].imshow(im1[2])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
