{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BFR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "The following is a from-scratch implementation of BFR and k-means and a performance comparison. The code also contains functions for generating datasets that have distinct clusters and artificial noise in them. All experiments and conductions were done on these datasets and might vary on different ones.\n",
    "\n",
    "**Initialization** - Regarding the initialization phase of the BFR algorithm, some sources suggest clustering the first batch using an in-memory algorithm and creating a Discard Set (DS) from sufficiently large clusters. Even though this method has the advantage of a variable number of clusters, I found this method to have worse SSE and Silhouette scores compared to a method that clusters a subset to a fixed number of clusters from the first batch and creates DS from this clustering. While this method yields a fixed number of clusters, it generally produces better results, provided that the number of clusters (K) is chosen correctly.\n",
    "\n",
    "**Dimensionality** - BFR was designed to cluster data in a high-dimensional space, so the results are much better for higher dimensions datasets. When comparing BFR with k-means on the 2D dataset, BFR was able to achieve only 30% of the k-means SSE score and about 80% of the Silhouette score. When dealing with 10+ dimensional datasets, the BFR was able to have scores similar to k-means.\n",
    "\n",
    "**Granularity of k-means** - Implemented k-means has an ending condition defined as a number of samples that changed cluster must be under 'KEND'. Variable 'KEND' plays a significant role not only in speed but also final result. With smaller 'KEND', the algorithm runs longer but results in more distinct clusters. Because BFR calls k-means on smaller batches, we can achieve decent speed even with smaller 'KEND'. On bigger datasets, BFR can achieve several times speed-up compared to k-means.\n",
    "\n",
    "**Dataset size** - BFR was designed to handle big datasets, so the performance on small datasets is not that great.\n",
    "\n",
    "**Batch size** - Bigger batch sizes lead to better results, but the difference starts to diminish with bigger datasets. Bigger batch sizes also increase the wall-clock time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install seaborn==0.12.1 matplotlib==3.6.0 scipy==1.9.1 sklearn==1.1.2 numpy==1.23.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "generating datasets to play with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.qmc import Sobol\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def sample_sobol(d, n):\n",
    "    sampler = Sobol(d)\n",
    "    return sampler.random(n)\n",
    "\n",
    "\n",
    "def sample_random(d, n):\n",
    "    samples = []\n",
    "    for i in range(n):\n",
    "        sample = []\n",
    "        for s in range(d):\n",
    "            sample.append(random.random())\n",
    "        samples.append(sample)\n",
    "\n",
    "    return np.array(samples)\n",
    "\n",
    "\n",
    "def euclidean_dist(a, b):\n",
    "    return np.sqrt(((a - b)**2).sum())\n",
    "\n",
    "\n",
    "def generate_dataset(d, n, dsens):\n",
    "    \"\"\"\n",
    "    First sample from quasi random Sobol\n",
    "    then choose centers and remove samples far from any center\n",
    "    then sample random points to create noise \n",
    "    \"\"\"\n",
    "    samples = sample_sobol(d, n*10)\n",
    "\n",
    "    nclusters = random.randint(8, 11)\n",
    "    clusters = []\n",
    "    while len(clusters) != nclusters:\n",
    "        cr = np.array([random.random() for _ in range(d)])\n",
    "        distinct = True\n",
    "        for cluster in clusters:\n",
    "            if euclidean_dist(cr, cluster) < 0.25:\n",
    "                distinct = False\n",
    "                break\n",
    "        if distinct:\n",
    "            clusters.append(cr)\n",
    "\n",
    "    _samples = []\n",
    "    for cluster in clusters:\n",
    "        for sample in samples:\n",
    "            append = True\n",
    "            for s in range(d):\n",
    "                sen = random.random()/random.randint(*dsens)*d\n",
    "                if math.sqrt((cluster[s] - sample[s])**2) > sen:\n",
    "                    append = False\n",
    "                    break\n",
    "\n",
    "            if append:\n",
    "                _samples.append(sample)\n",
    "            if len(_samples) >= (int)(n * 0.985):\n",
    "                break\n",
    "\n",
    "    _samples = np.array([*_samples, *sample_random(d, (int)(n*0.015))])\n",
    "    return _samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_samples(samples, cluster_ids):\n",
    "    sns.jointplot(x=samples[:, 0], y=samples[:, 1], hue=cluster_ids, s=5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_with_centers(centers):\n",
    "    # just for drawing centers as points in plot\n",
    "    for i in range(len(centers)):\n",
    "        _samples = np.append(_samples, [centers[i]], axis=0)\n",
    "        _cluster_ids = np.append(_cluster_ids, K + 1)\n",
    "    plot_samples(_samples, _cluster_ids)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmeans + BFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "def kmeans(data_set, centers):\n",
    "    dims = len(data_set[0])\n",
    "    cluster_ids = []\n",
    "    clusters = [[] for _ in range(len(centers))]\n",
    "\n",
    "    for i, point in enumerate(data_set):\n",
    "        min_dis = euclidean_dist(point, centers[0])\n",
    "        min_index = 0\n",
    "        for j, center in enumerate(centers[1:]):\n",
    "            dis = euclidean_dist(point, center)\n",
    "            if dis < min_dis:\n",
    "                min_dis = dis\n",
    "                min_index = j + 1\n",
    "        clusters[min_index].append(data_set[i])\n",
    "        cluster_ids.append(min_index)\n",
    "\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        if len(cluster) > 0:\n",
    "            sums = [0 for _ in range(dims)]\n",
    "\n",
    "            for point in cluster:\n",
    "                for d in range(dims):\n",
    "                    sums[d] += point[d]\n",
    "\n",
    "            for j in range(dims):\n",
    "                sums[j] /= len(cluster)\n",
    "\n",
    "            centers[i] = sums\n",
    "\n",
    "    return centers, clusters, cluster_ids\n",
    "\n",
    "\n",
    "def kmeans_fit(samples, K):\n",
    "    \"\"\"\n",
    "    kmeans that ends if less than KEND samples changed cluster\n",
    "    \"\"\"\n",
    "    centers = random.sample(list(samples), K)\n",
    "    cluster_ids = []\n",
    "\n",
    "    it = 0\n",
    "    while True:\n",
    "        centers, clusters, _cluster_ids = kmeans(samples, centers)\n",
    "        diff = sum([0 if _cluster_ids[i] == cluster_ids[i] else 1 for i in range(len(cluster_ids))])\n",
    "        cluster_ids = _cluster_ids\n",
    "\n",
    "        if diff < KEND and it != 0:\n",
    "            return clusters, _cluster_ids, centers\n",
    "            break\n",
    "\n",
    "        it += 1\n",
    "\n",
    "\n",
    "def std_dev(points: np.array):\n",
    "    if len(points) == 1:\n",
    "        return 1\n",
    "    u = points.mean()\n",
    "    return np.sqrt(((points - u)**2).sum()/len(points))\n",
    "\n",
    "\n",
    "def load_batch(samples, i):\n",
    "    return samples[i * BS:(i+1) * BS]\n",
    "\n",
    "\n",
    "def mahal_dist(point, summary):\n",
    "    xc = (point - (summary[1]/summary[0]))\n",
    "    var = summary[2] / summary[0] - (summary[1] / summary[0])**2\n",
    "    stddev = np.sqrt(var) + 1e-7  # just to be sure we don't divide by zero\n",
    "    return np.sqrt(((xc/stddev)**2).sum())\n",
    "\n",
    "\n",
    "def is_cluster_small(cluster, nosamples):\n",
    "    return len(cluster) < SMALL_CLUSTER_RATIO * nosamples\n",
    "\n",
    "\n",
    "def is_cluster_smallv2(cluster):\n",
    "    return len(cluster) < SMALL_CLUSTER\n",
    "\n",
    "\n",
    "def is_cluster_big(cluster, nosamples):\n",
    "    return len(cluster) > BIG_CLUSTER_RATIO * nosamples\n",
    "\n",
    "\n",
    "def is_cluster_bigv2(cluster):\n",
    "    return len(cluster) > BIG_CLUSTER\n",
    "\n",
    "\n",
    "def init_summary(cluster):\n",
    "    # return summary for cluster\n",
    "\n",
    "    dims = len(cluster[0])\n",
    "    n = len(cluster)\n",
    "    _sum = np.zeros(dims)\n",
    "    _sumq = np.zeros(dims)\n",
    "\n",
    "    for point in cluster:\n",
    "        _sum += point\n",
    "        _sumq += point**2\n",
    "\n",
    "    return [n, _sum, _sumq]\n",
    "\n",
    "\n",
    "def update_summary(summary, cluster):\n",
    "    # update summary with cluster\n",
    "    summary[0] += len(cluster)\n",
    "    for point in cluster:\n",
    "        summary[1] += point\n",
    "        summary[2] += point**2\n",
    "    return summary\n",
    "\n",
    "\n",
    "def var_comb_cluster(a, b):\n",
    "    # compute variance for combined a, b clusters\n",
    "    n = a[0] + b[0]\n",
    "    s = a[1] + b[1]\n",
    "    sq = a[2] + b[2]\n",
    "\n",
    "    return (sq/n) - (s/n)**2\n",
    "\n",
    "\n",
    "def merge_cs(CS):\n",
    "    if len(CS) > 1:\n",
    "        for i in range(len(CS)):\n",
    "            j = i\n",
    "            while j < len(CS):\n",
    "                if (CS_MERGE_TRESH > var_comb_cluster(CS[i], CS[j])).all():\n",
    "                    # merge CS\n",
    "                    CS[i][0] += CS[j][0]\n",
    "                    CS[i][1] += CS[j][1]\n",
    "                    CS[i][2] += CS[j][2]\n",
    "                    del CS[j]\n",
    "                    j -= 1\n",
    "                j += 1\n",
    "\n",
    "\n",
    "def init_bfr(_samples, DS, CS, RS):\n",
    "    rest_points = []\n",
    "\n",
    "    if True:\n",
    "        d = (int)(len(_samples)/10)\n",
    "        subset = _samples[0:d]\n",
    "        clusters, cluster_ids, _ = kmeans_fit(subset, K)\n",
    "        for cluster in clusters:\n",
    "            DS.append(init_summary(cluster))\n",
    "        rest_points = _samples[d:]\n",
    "    else:\n",
    "        # slightly different initialization\n",
    "        # hard to find the right hyperparams, but variable n of clusters\n",
    "        clusters, cluster_ids, _ = kmeans_fit(_samples, KK)\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            if is_cluster_bigv2(cluster):\n",
    "                DS.append(init_summary(cluster))\n",
    "            else:\n",
    "                rest_points.extend(cluster)\n",
    "\n",
    "    if len(rest_points) > 0:\n",
    "        clusters, cluster_ids, _ = kmeans_fit(rest_points, min(KK, len(rest_points)))\n",
    "        for cluster in clusters:\n",
    "            if is_cluster_smallv2(cluster):\n",
    "                RS.extend(cluster)\n",
    "            else:\n",
    "                CS.append(init_summary(cluster))\n",
    "\n",
    "\n",
    "def merge_all(DS, CS, RS):\n",
    "    if len(DS) > 0:\n",
    "        # merge points into the closest DS\n",
    "        for sample in RS:\n",
    "            min_dist = float(\"inf\")\n",
    "            for dsi, ds in enumerate(DS):\n",
    "                dist = mahal_dist(sample, ds)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    minin = dsi\n",
    "            DS[minin] = update_summary(DS[minin], [sample])\n",
    "\n",
    "        # merge cs into the closest DS\n",
    "        for cs in CS:\n",
    "            min_dist = float(\"inf\")\n",
    "            for dsi, ds in enumerate(DS):\n",
    "                dist = ((ds[1]/ds[0] - cs[1]/cs[0])**2).sum()\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    minin = dsi\n",
    "\n",
    "            DS[minin][0] += cs[0]\n",
    "            DS[minin][1] += cs[1]\n",
    "            DS[minin][2] += cs[2]\n",
    "\n",
    "\n",
    "def cluster_rs(CS, RS):\n",
    "    if len(RS) > 0:\n",
    "        clusters, cluster_ids, _ = kmeans_fit(RS, min(KK, len(RS)))\n",
    "        RS = []\n",
    "        for cluster in clusters:\n",
    "            if is_cluster_smallv2(cluster):\n",
    "                RS.extend(cluster)\n",
    "            else:\n",
    "                CS.append(init_summary(cluster))\n",
    "\n",
    "\n",
    "def classify_samples(samples, DS, CS, RS):\n",
    "    ndims = len(samples[0])\n",
    "    for sample in samples:\n",
    "        is_merged = False\n",
    "        for i, summary in enumerate(DS):\n",
    "            if mahal_dist(sample, summary) < math.sqrt(ndims):\n",
    "                DS[i] = update_summary(DS[i], sample)\n",
    "                is_merged = True\n",
    "                break\n",
    "        if not is_merged:\n",
    "            for i, summary in enumerate(CS):\n",
    "                if mahal_dist(sample, summary) < math.sqrt(ndims):\n",
    "                    CS[i] = update_summary(CS[i], sample)\n",
    "                    is_merged = True\n",
    "                    break\n",
    "        if not is_merged:\n",
    "            RS.append(sample)\n",
    "\n",
    "\n",
    "def bfr(samples, debugging=False):\n",
    "    DS = []\n",
    "    CS = []\n",
    "    RS = []\n",
    "\n",
    "    _samples = load_batch(samples, 0)\n",
    "    init_bfr(_samples, DS, CS, RS)\n",
    "\n",
    "    it = 1\n",
    "    _samples = load_batch(samples, it)\n",
    "    while len(_samples) > 0:\n",
    "        classify_samples(_samples, DS, CS, RS)\n",
    "        cluster_rs(CS, RS)\n",
    "        merge_cs(CS)\n",
    "\n",
    "        it += 1\n",
    "        _samples = load_batch(samples, it)\n",
    "\n",
    "    merge_all(DS, CS, RS)\n",
    "\n",
    "    clss = []\n",
    "    if debugging and len(DS) > 0:\n",
    "        # assign samples to clusters\n",
    "        clusters = [[] for _ in range(len(DS))]\n",
    "        for sample in samples:\n",
    "            min_dist = float('inf')\n",
    "            minin = 0\n",
    "            for dsi, ds in enumerate(DS):\n",
    "                dist = mahal_dist(sample, ds)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    minin = dsi\n",
    "\n",
    "            clss.append(minin)\n",
    "            clusters[minin].append(sample)\n",
    "\n",
    "        for i in range(len(clusters)):\n",
    "            clusters[i] = np.array(clusters[i])\n",
    "\n",
    "        centers = [d[1]/d[0] for d in DS]\n",
    "\n",
    "        return clusters, clss, centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize(samples, RS, CS, DS):\n",
    "    colors = []\n",
    "    for sample in samples:\n",
    "        if np.isin(sample, RS).any():\n",
    "            colors.append(1)\n",
    "        else:\n",
    "            min_dist = float('inf')\n",
    "            colors.append(-1)\n",
    "            idx = len(colors) - 1\n",
    "            for csi, cs in enumerate(CS):\n",
    "                dist = mahal_dist(sample, cs)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    colors[idx] = 2\n",
    "            for dsi, ds in enumerate(DS):\n",
    "                dist = mahal_dist(sample, ds)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    colors[idx] = 3\n",
    "\n",
    "    print(\"RS\", len(RS))\n",
    "    print(\"CS\", len(CS))\n",
    "    print(\"DS\", len(DS))\n",
    "\n",
    "    plot_samples(samples, colors)\n",
    "\n",
    "    # plot_samples(_samples, cluster_ids)\n",
    "\n",
    "\n",
    "def sse(clusters, centers):\n",
    "    _sse = 0\n",
    "    for i, centroid in enumerate(centers):\n",
    "        for sample in clusters[i]:\n",
    "            for d in range(len(centroid)):\n",
    "                _sse += (sample[d] - centroid[d]) ** 2\n",
    "    return _sse\n",
    "\n",
    "\n",
    "def test_bfr_kmeans(n):\n",
    "    bfr_time = bfr_sse = bfr_sil = 0\n",
    "    kmeans_time = kmeans_sse = kmeans_sil = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        print(f\"test {i+1}\")\n",
    "        samples = generate_dataset(TEST_DIM, N, TEST_DSENS)\n",
    "\n",
    "        st = time.time()\n",
    "        clusters, clss, centers = bfr(samples, True)\n",
    "        bfr_time += time.time() - st\n",
    "        bfr_sse += sse(clusters, centers)\n",
    "        bfr_sil += metrics.silhouette_score(samples, clss) + 1\n",
    "\n",
    "        st = time.time()\n",
    "        clusters, clss, centers = kmeans_fit(samples, K)\n",
    "        kmeans_time += time.time() - st\n",
    "        kmeans_sse += sse(clusters, centers)\n",
    "        kmeans_sil += metrics.silhouette_score(samples, clss) + 1\n",
    "\n",
    "    print(\"bfr time\", bfr_time/n)\n",
    "    print(\"bfr sse\", bfr_sse/n)\n",
    "    print(\"bfr sil\", bfr_sil/n)\n",
    "    print(\"----\")\n",
    "    print(\"kmeans time\", kmeans_time/n)\n",
    "    print(\"kmeans sse\", kmeans_sse/n)\n",
    "    print(\"kmeans sil\", kmeans_sil/n)\n",
    "    print(\"----\")\n",
    "    # bfr better with 1+\n",
    "    print(\"bfr/kmeans perf time\", kmeans_time/bfr_time)\n",
    "    print(\"bfr/kmeans perf sse\", kmeans_sse/bfr_sse)\n",
    "    print(\"bfr/kmeans perf sil\", bfr_sil/kmeans_sil)\n",
    "\n",
    "\n",
    "def bfr_dataset(path):\n",
    "    samples = np.genfromtxt(path, delimiter=',')\n",
    "\n",
    "    st = time.time()\n",
    "    clusters, clss, centers = kmeans_fit(samples, K)\n",
    "    # clusters, clss, centers = bfr(samples, True)\n",
    "\n",
    "    print(time.time() - st)\n",
    "    print(len(samples))\n",
    "    print(len(clusters))\n",
    "    print(sse(clusters, centers))\n",
    "    print(metrics.silhouette_score(samples, clss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIM = 15\n",
    "TEST_DSENS = [6, 13]\n",
    "\n",
    "N = 40000  # number of samples\n",
    "BS = 20000  # BFR batch size\n",
    "KEND = 500  # BFR kmeans ends if less than KEND samples changed cluster\n",
    "\n",
    "K = 8\n",
    "KK = 8  # K for the inner kmeans in bfr\n",
    "\n",
    "BIG_CLUSTER_RATIO = 0.05\n",
    "BIG_CLUSTER = 100\n",
    "\n",
    "SMALL_CLUSTER = 5\n",
    "SMALL_CLUSTER_RATIO = 0.01\n",
    "\n",
    "CS_MERGE_TRESH = 0.02\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # samples = generate_dataset(2, 1000, [7, 9])\n",
    "    # plot_samples(samples, None)\n",
    "\n",
    "    num_tests = 2\n",
    "    test_bfr_kmeans(num_tests)\n",
    "\n",
    "    # bfr_dataset('2d-dataset.csv')\n",
    "    # bfr_dataset('13d-dataset.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4525bb9afa5b11e91ff8883ff1427ebcb57df9afe31e89541862e2caa0e84c72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
