{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#! pip install git+https://github.com/MIR-MU/pv211-utils.git\n",
    "#! pip install transformers sentence-transformers\n",
    "#! pip install torch==1.9.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html --no-cache-dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These is the initial material for publication [Diverse Semantics Representation is King](https://ceur-ws.org/Vol-3180/paper-02.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ranked Retrieval System\n",
    "\n",
    "The system computes cosine similarity between query embedding and answer embeddings for each query, and top n answers are further re-ranked using Cross-Encoders.\n",
    "I tried to use no cross-encoder, single CE, and ensemble CEs (weighted average). Introducing CE improved the performance significantly. Using one smaller CE alongside the main one didn't really improve the best score but might be a more stable approach.\n",
    "\n",
    "The cross-encoders were fine-tuned, and final models can be downloaded at https://drive.google.com/drive/folders/1eiZl8ftAR1rYp2TFf6SZ2VF2f6HVa0mz?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_format = \"text+latex\"\n",
    "retriever_model_id = \"all-MiniLM-L12-v2\"\n",
    "reranker_model_id = \"sentence-transformers/stsb-roberta-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pv211_utils.arqmath.loader import load_answers\n",
    "from pv211_utils.arqmath.entities import ArqmathAnswerBase, ArqmathQueryBase, ArqmathQuestionBase\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class Answer(ArqmathAnswerBase):\n",
    "    def __init__(self, document_id: str, body: str, upvotes: int, is_accepted: bool):\n",
    "        super().__init__(document_id, body, upvotes, is_accepted)\n",
    "        self.representation = ' '.join(self.body.split())  # remove multiple spaces\n",
    "\n",
    "\n",
    "class Question(ArqmathQuestionBase):\n",
    "    def __init__(self, document_id: str, title: str, body: str, tags: List[str],\n",
    "                 upvotes: int, views: int, answers: List[Answer]):\n",
    "        super().__init__(document_id, title, body, tags, upvotes, views, answers)\n",
    "        self.representation = self.title + ' ' + ', '.join(tags).replace(\"-\", \" \") + \". \" + self.body\n",
    "\n",
    "\n",
    "class Query(ArqmathQueryBase):\n",
    "    def __init__(self, query_id: int, title: str, body: str, tags: List[str]):\n",
    "        super().__init__(query_id, title, body, tags)\n",
    "        self.representation = self.title + ' ' + ', '.join(tags).replace(\"-\", \" \") + \". \" + self.body\n",
    "\n",
    "\n",
    "answers = load_answers(text_format, Answer, cache_download=f'/var/tmp/pv211/arqmath2020_answers_{text_format}.json.gz')\n",
    "\n",
    "tanswers = [answers[ans].body.lower() for ans in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from numpy.linalg import norm\n",
    "\n",
    "retriever_model = SentenceTransformer(retriever_model_id, device=\"cuda\")\n",
    "retriever_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    answers_embeddings = retriever_model.encode(tanswers, convert_to_tensor='pt', batch_size=64)\n",
    "\n",
    "answers_embeddings = answers_embeddings.detach().cpu()\n",
    "answers_embeddings_np = answers_embeddings.numpy()\n",
    "norm_answers_embedding = [norm(embedding) for embedding in answers_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm_answers_embedding = [norm(embedding) for embedding in answers_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker_model = CrossEncoder(reranker_model_id, num_labels=1)\n",
    "\n",
    "# model.model.load_state_dict(torch.load('./ce_berta_modelv22_x120.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pv211_utils.arqmath.irsystem import ArqmathIRSystemBase\n",
    "from tqdm import tqdm\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class IRSystem(ArqmathIRSystemBase):\n",
    "    def __init__(self):\n",
    "        self.answers = list(answers.values())\n",
    "\n",
    "    def _get_query_embedding(self, query: Query):\n",
    "        return retriever_model.encode(query.representation)\n",
    "\n",
    "    def search(self, query: Query):\n",
    "        query_embedding = self._get_query_embedding(query)\n",
    "        query_embedding_norm = norm(query_embedding)\n",
    "\n",
    "        cos_sims = [dot(query_embedding, answers_embeddings_np[i]) / (query_embedding_norm * norm_answers_embedding[i])\n",
    "                    for i in range(len(answers_embeddings))]\n",
    "\n",
    "        predictions_retriever = np.array(cos_sims).argsort()[::-1]\n",
    "        \n",
    "        # sorting in batches\n",
    "        batch_indexes = [4, 8, 12, 16, 32, 64, 128]\n",
    "        for i in range(len(batch_indexes)):\n",
    "            start_index = 0 if i == 0 else batch_indexes[i - 1]\n",
    "            top_set = [[query.representation, self.answers[predictions_retriever[j]].representation]\n",
    "                       for j in range(start_index, batch_indexes[i])]\n",
    "            reranker_preds = reranker_model.predict(top_set, batch_size=16)\n",
    "            top_preds = np.array(reranker_preds).argsort()[::-1]\n",
    "\n",
    "            for top_doc in top_preds:\n",
    "                yield self.answers[predictions_retriever[top_doc + start_index]]\n",
    "\n",
    "        for doc in predictions_retriever:\n",
    "            yield self.answers[doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pv211_utils.arqmath.loader import load_queries, load_judgements\n",
    "from pv211_utils.evaluation_metrics import mean_average_precision\n",
    "\n",
    "test_queries = load_queries(text_format, Query, year=2021)\n",
    "test_judgements = load_judgements(test_queries, answers, year=2021)\n",
    "map_score = mean_average_precision(IRSystem(), test_queries, test_judgements, 10, 1)\n",
    "map_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset\n",
    "\n",
    "At each epoch, we create a list of examples (query, answer, relevance). The relevant answer is returned with prob. 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "class ArqMathDataset(Dataset):\n",
    "    def __init__(self, queries: OrderedDict, judgements: OrderedDict, answers: OrderedDict):\n",
    "        self.queries = list(queries.values())\n",
    "        self.answers = list(answers.values())\n",
    "        \n",
    "        self.judgements = {}\n",
    "        self.ids = set()\n",
    "\n",
    "        for judgement in judgements:\n",
    "            q = judgement[0].query_id\n",
    "            self.judgements.setdefault(q, []).append(judgement[1])\n",
    "            self.ids.add(q)\n",
    "        self.ids = sorted(self.ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    @classmethod\n",
    "    def random_crop(self, string: str) -> str:\n",
    "        \"\"\"Do a random crop of a string that fits to a transformer\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        string: str\n",
    "            string to be cropped\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        cropped string: str\n",
    "        \"\"\"\n",
    "        lst = string.split(\" \")\n",
    "        mx = len(lst) - (MAX_SEQ_LEN + 1)  # max in seq lenght is 512 for most transformers\n",
    "\n",
    "        # if the text is not that much longer just leave it like that \n",
    "        if mx > 10:\n",
    "            st = random.randrange(0, mx)\n",
    "            return \" \".join(lst[st:st+MAX_SEQ_LEN])\n",
    "        return string\n",
    "\n",
    "    def __getitem__(self, idx: int) -> list:\n",
    "        \"\"\"Take text of query, random answer and their relevance\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        idx: int\n",
    "            index of query\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        query text: str\n",
    "        answer text: str\n",
    "        relevance: Tensor\n",
    "        \"\"\"\n",
    "        query = self.queries[idx]\n",
    "        answer = None\n",
    "        rel = torch.Tensor([[1]])\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            answer = random.choice(self.judgements[query.query_id])\n",
    "        else:\n",
    "            rel = torch.Tensor([[0]])\n",
    "            while True:\n",
    "                rnd = random.randint(0, len(self.answers))\n",
    "                answer = self.answers[rnd]\n",
    "                if answer not in self.judgements[query.query_id]:\n",
    "                    break\n",
    "\n",
    "        query_str = self.random_crop(query.body)\n",
    "        answer_str = self.random_crop(answer.body)\n",
    "\n",
    "        return InputExample(texts=[query_str, answer_str], label=rel.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from pv211_utils.arqmath.loader import load_queries, load_judgements\n",
    "\n",
    "smaller_train_queries = load_queries(text_format, Query, 'train', year=2020)\n",
    "smaller_validation_queries = load_queries(text_format, Query, 'validation', year=2020)\n",
    "\n",
    "train_queries = OrderedDict(chain(smaller_train_queries.items(), smaller_validation_queries.items()))\n",
    "validation_queries = load_queries(text_format, Query, 'test', year=2020)\n",
    "\n",
    "bigger_train_queries = OrderedDict(chain(train_queries.items(), validation_queries.items()))\n",
    "\n",
    "smaller_train_judgements = load_judgements(smaller_train_queries, answers, 'train', year=2020)\n",
    "smaller_validation_judgements = load_judgements(smaller_validation_queries, answers, 'validation', year=2020)\n",
    "\n",
    "train_judgements = smaller_train_judgements | smaller_validation_judgements\n",
    "validation_judgements = load_judgements(validation_queries, answers, 'test', year=2020)\n",
    "\n",
    "bigger_train_judgements = train_judgements | validation_judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "\n",
    "val_dataset = ArqMathDataset(validation_queries, validation_judgements, answers)\n",
    "\n",
    "val_examples = []\n",
    "for j in range(20):\n",
    "    for data in val_dataset:\n",
    "        val_examples.append(data)\n",
    "\n",
    "ce_eval = CEBinaryClassificationEvaluator.from_input_examples(val_examples)\n",
    "print(ce_eval(reranker_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "from torch.optim import AdamW\n",
    "\n",
    "model = CrossEncoder(reranker_model_id, num_labels=1)\n",
    "\n",
    "train_dataset = ArqMathDataset(train_queries, train_judgements, answers)\n",
    "ce_eval = CEBinaryClassificationEvaluator.from_input_examples(val_examples)\n",
    "\n",
    "def fit():\n",
    "    train_examples = []\n",
    "\n",
    "    # 50 samples per question\n",
    "    for j in range(50):\n",
    "        for data in train_dataset:\n",
    "            train_examples.append(data)\n",
    "\n",
    "    train_dataloader = DataLoader(train_examples, batch_size=32)\n",
    "    model.fit(train_dataloader, show_progress_bar=True, optimizer_class=AdamW)\n",
    "\n",
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    fit()\n",
    "    print(f\"[{i}/{epochs}] val_eval: {ce_eval(model)}\")\n",
    "\n",
    "    if i % 2 == 0:\n",
    "        torch.save(model.model.state_dict(), f'./model_{i}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "model.model.load_state_dict(torch.load('./model_0.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4525bb9afa5b11e91ff8883ff1427ebcb57df9afe31e89541862e2caa0e84c72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
