{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bookcorpus (/home/batman/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datasets\n",
    "\n",
    "small_book_corpus = datasets.load_dataset(\"bookcorpus\", split=\"train[:1%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing tokenizer\n",
      "creating vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 46.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from itertools import pairwise\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class BPETokenizer():\n",
    "    SPACE = \"Ġ\"  # use gpt2 like space representation\n",
    "\n",
    "    def __init__(self, sentences, vocab_size=100):\n",
    "        self.vocab_size = vocab_size\n",
    "        self._init_tokenizer(sentences)\n",
    "        self._create_vocab()\n",
    "\n",
    "    def _init_tokenizer(self, sentences):\n",
    "        print(\"initializing tokenizer\")\n",
    "        texts = \" \".join(sentences)\n",
    "        texts = re.sub(r\" \", f\" {self.SPACE}\", texts)\n",
    "\n",
    "        self.chars = list(set(texts))\n",
    "        self.vocab = list(self.chars)\n",
    "\n",
    "        words = Counter(texts.split(\" \"))\n",
    "        splits = [[c for c in word] for word in words.keys()]\n",
    "        self.corpus = [list(row) for row in zip(words.values(), words.keys(), splits)\n",
    "                       if len(row[2]) > 1]  # remove single chars\n",
    "\n",
    "    def _get_most_freq_pair(self):\n",
    "        pairs = defaultdict(int)\n",
    "        for cnt, _, split in self.corpus:\n",
    "            for pair in pairwise(split):\n",
    "                pairs[pair] += cnt\n",
    "        return max(pairs.items(), key=lambda x: x[1])\n",
    "\n",
    "    def _merge_pair(self, pair):\n",
    "        pair_str = \"\".join(pair[0])\n",
    "        for i, row in enumerate(self.corpus):\n",
    "            if pair_str not in row[1]:\n",
    "                continue\n",
    "            split = row[2]\n",
    "            j = 0\n",
    "            while j + 1 < len(split):\n",
    "                if split[j] == pair[0][0] and split[j + 1] == pair[0][1]:\n",
    "                    split = split[:j] + [pair_str] + split[j + 2:]\n",
    "                j += 1\n",
    "\n",
    "            self.corpus[i][2] = split\n",
    "\n",
    "    def _create_vocab(self):\n",
    "        print(\"creating vocabulary\")\n",
    "        with tqdm(total=self.vocab_size) as prog_bar:\n",
    "            prog_bar.update(len(self.vocab))\n",
    "            while len(self.vocab) != self.vocab_size:\n",
    "                pair = self._get_most_freq_pair()\n",
    "                self._merge_pair(pair)\n",
    "                self.vocab.append(\"\".join(pair[0]))\n",
    "                prog_bar.update(1)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = text.replace(\" \", self.SPACE)\n",
    "        split = list(text)\n",
    "\n",
    "        for v in self.vocab[len(self.chars):]:\n",
    "            i = 0\n",
    "            if v not in text:\n",
    "                continue\n",
    "            while i + 1 < len(split):\n",
    "                if split[i] + split[i + 1] == v:\n",
    "                    split = split[:i] + [v] + split[i + 2:]\n",
    "                i += 1\n",
    "        return split\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.vocab.index(c) for c in self.tokenize(text)]\n",
    "\n",
    "    def decode(self, encoding):\n",
    "        enc_text = \"\".join([self.vocab[enc] for enc in encoding]).replace(self.SPACE, \" \")\n",
    "        return enc_text\n",
    "\n",
    "small_book_texts = small_book_corpus[:][\"text\"]\n",
    "tokenizer = BPETokenizer(small_book_texts, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 65, 82, 5, 23, 80, 3, 2, 81, 67, 79, 45, 45, 38]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"she is reading a book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "EMB_SIZE = 64\n",
    "CONTEXT_SIZE = 32\n",
    "\n",
    "\n",
    "class BookDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer):\n",
    "        self.chunk_size = 3000\n",
    "        self.chunk_ind = 0\n",
    "        self.text = \" \".join(sentences).split(\" \")\n",
    "        self.lags = 32\n",
    "        self.tokenizer = tokenizer\n",
    "        self.load_new_chunk()\n",
    "\n",
    "    def load_new_chunk(self):\n",
    "        chunk = \" \".join(self.text[self.chunk_ind:self.chunk_ind+self.chunk_size])\n",
    "        self.encoding = self.tokenizer.encode(chunk)\n",
    "        self.chunk_ind += self.chunk_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoding) + self.lags\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.encoding[idx:idx + self.lags], dtype=torch.long)\n",
    "        y = torch.tensor([self.encoding[idx + self.lags]], dtype=torch.long)\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dataset = BookDataset(small_book_texts, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataLoader:\n",
    "    def __init__(self, dataset, bs, device):\n",
    "        self.dataset = dataset\n",
    "        self.chunk_size = int(len(dataset) / bs)\n",
    "        self.istep = 0\n",
    "        self.bs = bs\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.chunk_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.chunk_size):\n",
    "            xs, ys = zip(*[self.dataset[i + self.istep] for i in range(self.bs)])\n",
    "            \n",
    "            self.istep += 1\n",
    "            yield torch.stack(xs).to(self.device), torch.stack(ys).to(self.device)\n",
    "        self.dataset.load_new_chunk()\n",
    "\n",
    "device = \"cuda\"\n",
    "bs = 1024\n",
    "train_bookloader = BookDataLoader(book_dataset, bs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[65, 14, 23, 99, 11, 43, 80, 67, 18,  3, 14, 11, 50, 99, 11, 79, 64, 43,\n",
       "         17,  2, 81, 87, 74, 17,  2, 23, 18, 14, 45, 12,  3, 12]],\n",
       "       device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# just pick a random sentence as input for generating\n",
    "starter_gen = torch.Tensor([tokenizer.encode(random.choice(small_book_texts))[:32]]).int().to(device)\n",
    "starter_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from transformer import TransformerModel\n",
    "import wandb\n",
    "\n",
    "run = wandb.init(name=\"init test\", project=\"midwrit\", reinit=True)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "model = TransformerModel(64, 2, 4, 32, tokenizer.vocab_size).to(device)\n",
    "optim = Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "tloss = 0\n",
    "with run:\n",
    "    for epoch in range(epochs):\n",
    "        for i, [x, y] in enumerate(train_bookloader):\n",
    "            optim.zero_grad()\n",
    "\n",
    "            pred = model(x)[:, -1, :]\n",
    "\n",
    "            loss = loss_fn(pred, y.squeeze())\n",
    "            loss.backward()\n",
    "            tloss += loss.item()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "        if epoch % 20 == 0 and epoch > 0:\n",
    "            print(f\"[{epoch}/{epochs}]: {tloss}\")\n",
    "            run.log({\"train_loss\": tloss})\n",
    "            tloss = 0\n",
    "\n",
    "            print(tokenizer.decode(model.generate_tokens(starter_gen, 200, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ou intiong . she gains ? she post . you ? '\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model.generate_tokens(starter_gen, 200, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
