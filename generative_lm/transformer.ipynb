{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class LexicapDataset(Dataset):\n",
    "    def __init__(self, path: str, lags=5):\n",
    "        chars = [chr(i) for i in range(65, 91)]  # A-Z\n",
    "        chars.extend([chr(i) for i in range(97, 123)])  # a - z\n",
    "        # let's use # as start token, and _ as pad\n",
    "        chars.extend([\" \", \",\", \".\", \"'\", \"_\", \"#\"])\n",
    "\n",
    "        self.itos = {i: x for i, x in enumerate(chars)}\n",
    "        self.stoi = {x: i for i, x in enumerate(chars)}\n",
    "        self.vocab_size = len(chars) + 1  # +1 for undefined char\n",
    "\n",
    "        self.emb = np.array(self.get_texts(path))\n",
    "\n",
    "        self.lags = lags\n",
    "\n",
    "    def encode_char(self, c):\n",
    "        return self.stoi.get(c, self.vocab_size - 1)\n",
    "\n",
    "    def decode_char(self, oc):\n",
    "        return self.itos.get(oc, \"?\")\n",
    "\n",
    "    def decode_sentence(self, line: str):\n",
    "        return \"\".join([self.decode_char(c) for c in line])\n",
    "\n",
    "    def encode_sentence(self, line: str):\n",
    "        return [self.encode_char(c) for c in line]\n",
    "\n",
    "    def get_texts(self, path: str):\n",
    "        emb = []\n",
    "        for p in os.listdir(path):\n",
    "            if \"large\" in p:\n",
    "                lines = open(f\"{path}/{p}\").read().splitlines()\n",
    "                for i, line in enumerate(lines):\n",
    "                    if i % 3 == 0 and i > 0:\n",
    "                        emb.extend(self.encode_sentence(line.strip()))\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emb) - self.lags\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.emb[idx:idx + self.lags], dtype=torch.long)\n",
    "        y = torch.tensor([self.emb[idx + self.lags]], dtype=torch.long)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "train_lexicap = LexicapDataset(\"../data/vtt/train\", lags=32)\n",
    "test_lexicap = LexicapDataset(\"../data/vtt/test\", lags=32)  # episodes 300+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 33, 30, 52, 31, 40, 37, 37, 40, 48, 34, 39, 32, 52, 34, 44, 52, 26, 52, 28, 40, 39, 47, 30, 43, 44, 26, 45, 34, 40, 39, 52]\n",
      "[48]\n",
      "---\n",
      "[33, 30, 52, 31, 40, 37, 37, 40, 48, 34, 39, 32, 52, 34, 44, 52, 26, 52, 28, 40, 39, 47, 30, 43, 44, 26, 45, 34, 40, 39, 52, 48]\n",
      "[34]\n",
      "---\n",
      "[30, 52, 31, 40, 37, 37, 40, 48, 34, 39, 32, 52, 34, 44, 52, 26, 52, 28, 40, 39, 47, 30, 43, 44, 26, 45, 34, 40, 39, 52, 48, 34]\n",
      "[45]\n",
      "---\n",
      "[52, 31, 40, 37, 37, 40, 48, 34, 39, 32, 52, 34, 44, 52, 26, 52, 28, 40, 39, 47, 30, 43, 44, 26, 45, 34, 40, 39, 52, 48, 34, 45]\n",
      "[33]\n",
      "---\n",
      "[31, 40, 37, 37, 40, 48, 34, 39, 32, 52, 34, 44, 52, 26, 52, 28, 40, 39, 47, 30, 43, 44, 26, 45, 34, 40, 39, 52, 48, 34, 45, 33]\n",
      "[52]\n",
      "---\n",
      "[40, 37, 37, 40, 48, 34, 39, 32, 52, 34, 44, 52, 26, 52, 28, 40, 39, 47, 30, 43, 44, 26, 45, 34, 40, 39, 52, 48, 34, 45, 33, 52]\n",
      "[18]\n"
     ]
    }
   ],
   "source": [
    "for i , [x, y] in enumerate(train_lexicap):\n",
    "    print(x.tolist())\n",
    "    print(y.tolist())\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicapDataLoader:\n",
    "    def __init__(self, dataset, bs, device):\n",
    "        self.dataset = dataset\n",
    "        self.chunk_size = int(len(dataset) / bs)\n",
    "\n",
    "        self.bsi = [int(i * self.chunk_size) for i in range(bs)]\n",
    "        self.istep = 0\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.chunk_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.chunk_size):\n",
    "            xs, ys = zip(*[self.dataset[i + self.istep] for i in self.bsi])\n",
    "            \n",
    "            self.istep += 1\n",
    "            yield torch.stack(xs).to(self.device), torch.stack(ys).to(self.device)\n",
    "\n",
    "bs = 1024\n",
    "train_lexiloader = LexicapDataLoader(train_lexicap, bs, device)\n",
    "test_lexiloader = LexicapDataLoader(test_lexicap, bs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a conversation \n",
      "it's a very dynamic system.And g\n",
      "say you have to ask yourself and\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_lexiloader):\n",
    "    print(train_lexicap.decode_sentence(data[0][0].tolist()))\n",
    "    print(train_lexicap.decode_sentence(data[0][1].tolist()))\n",
    "    print(train_lexicap.decode_sentence(data[0][4].tolist()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, context_size, mask=None):\n",
    "        super().__init__()\n",
    "        self.in_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.context_size = context_size\n",
    "        head_size = emb_size // num_heads\n",
    "\n",
    "        self.q_layers = nn.ModuleList()\n",
    "        self.k_layers = nn.ModuleList()\n",
    "        self.v_layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_heads):\n",
    "            self.k_layers.append(nn.Linear(emb_size, head_size, bias=False))\n",
    "            self.q_layers.append(nn.Linear(emb_size, head_size, bias=False))\n",
    "            self.v_layers.append(nn.Linear(emb_size, head_size, bias=False))\n",
    "\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        self.ln = nn.LayerNorm(emb_size)\n",
    "\n",
    "        self._init_params()\n",
    "\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def _init_params(self):\n",
    "        for l in [*self.q_layers, *self.k_layers, *self.v_layers, self.projection]:\n",
    "            nn.init.xavier_uniform_(l.weight)\n",
    "\n",
    "    def _sdp_attention(self, q, k, v):\n",
    "        att_log = (q @ k.transpose(-2, -1))\n",
    "        if self.mask is not None:\n",
    "            att_log = att_log.masked_fill(self.mask == 0, float(\"-inf\"))\n",
    "        attention = F.softmax(att_log, dim=-1)\n",
    "        return attention @ v\n",
    "\n",
    "    def forward(self, x):\n",
    "        values = torch.tensor([], device=x.device)\n",
    "        for b in range(self.num_heads):\n",
    "            qkv = self.q_layers[b](x), self.k_layers[b](x), self.v_layers[b](x)\n",
    "            val = self._sdp_attention(*qkv)\n",
    "            values = torch.cat((val, values), dim=-1)\n",
    "\n",
    "        out = self.projection(values) + x\n",
    "        nout = self.ln(out)\n",
    "\n",
    "        return nout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, emb_size)\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(emb_size)\n",
    "        self.ffn.apply(self._init_params)\n",
    "\n",
    "    def _init_params(self, l):\n",
    "        if type(l) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(l.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ln(self.ffn(x) + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, context_size):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "\n",
    "        mask_tril = torch.tril(torch.ones(context_size, context_size))\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            MultiHeadAttention(emb_size, num_heads, context_size, mask_tril),\n",
    "            MultiHeadAttention(emb_size, num_heads, emb_size, mask_tril),\n",
    "            FFN(emb_size, emb_size * 8),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, emb_size, num_blocks, num_heads, context_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos_embedding = nn.Embedding(context_size, emb_size)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(emb_size, num_heads, context_size) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.out = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        te = self.token_embedding(x)\n",
    "        inp = te + self.pos_embedding(torch.arange(self.context_size).to(te.device))\n",
    "        blocks_out = self.blocks(inp)\n",
    "        return self.out(blocks_out)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, x, gen_len, top_k=5):\n",
    "        x = x[0]\n",
    "        sy = \"\"\n",
    "        for _ in range(gen_len):\n",
    "            pv, pi = self(x)[-1, :].topk(top_k)\n",
    "            p = F.softmax(pv, dim=-1)\n",
    "            q = pi[torch.multinomial(p, num_samples=1)]\n",
    "            \n",
    "            sy += train_lexicap.decode_char(q.item())\n",
    "            x = torch.cat((x[1:], q), dim=-1)\n",
    "\n",
    "        return x, sy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ES = 64 # embedding size\n",
    "NB = 2 # number of blocks\n",
    "NH = 4 # number of heads\n",
    "CS = 32 # size of context window\n",
    "VS = train_lexicap.vocab_size # vocab size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "import wandb\n",
    "run = wandb.init(name=\"final_version\", project=\"transformers\", reinit=True)\n",
    "\n",
    "model = TransformerModel(ES, NB, NH, CS, VS).to(device)\n",
    "optim = Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "tloss = 0\n",
    "with run:\n",
    "    for i, [x, y] in enumerate(train_lexiloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        pred = model(x)[:, -1, :]\n",
    "\n",
    "        loss = loss_fn(pred, y.squeeze())\n",
    "        loss.backward()\n",
    "        tloss += loss.item()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            print(f\"[{i}/{len(train_lexiloader)}]: {tloss}\")\n",
    "            run.log({\"train_loss\": tloss})\n",
    "\n",
    "            tloss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "x = train_lexicap[0][0].unsqueeze(0).cuda()\n",
    "_, gen_text = model.generate(x, 300, top_k=3)\n",
    "\n",
    "print(gen_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4525bb9afa5b11e91ff8883ff1427ebcb57df9afe31e89541862e2caa0e84c72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
